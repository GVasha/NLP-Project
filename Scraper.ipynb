{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BaA09R6fWfy7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Cell 1) Install dependencies\n",
        "# ============================================================\n",
        "!pip -q install beautifulsoup4 lxml readability-lxml requests tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHKZbXcgWpbC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\grego\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\__init__.py:113: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (6.0.0.post1)/charset_normalizer (3.3.2) doesn't match a supported version!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Cell 2) Imports + helpers\n",
        "# ============================================================\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import hashlib\n",
        "from urllib.parse import urljoin, urlparse, urldefrag\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "\n",
        "# readability-lxml helps remove menus/boilerplate when possible\n",
        "from readability import Document\n",
        "\n",
        "OUTPUT_DIR = \"/\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; TextExtractor/1.0; +https://colab.research.google.com/)\"\n",
        "}\n",
        "\n",
        "def safe_filename_from_url(url: str, max_len: int = 120) -> str:\n",
        "    \"\"\"Create a stable, filesystem-safe filename from a URL.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    base = (parsed.netloc + parsed.path).strip(\"/\")\n",
        "    base = base if base else \"root\"\n",
        "    base = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", base)\n",
        "    base = base[:max_len].strip(\"_\")\n",
        "    h = hashlib.sha1(url.encode(\"utf-8\")).hexdigest()[:10]\n",
        "    return f\"{base}_{h}.txt\"\n",
        "\n",
        "def fetch_html(url: str, timeout: int = 25) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    # requests usually guesses encoding; if not, this helps\n",
        "    r.encoding = r.apparent_encoding\n",
        "    return r.text\n",
        "\n",
        "def extract_visible_text(html: str, remove_header_footer: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Extract readable visible text from HTML.\n",
        "    Uses readability-lxml to get main content, then BeautifulSoup to textify.\n",
        "    Optionally removes header and footer elements before extraction.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # Remove headers and footers first (before readability)\n",
        "    if remove_header_footer:\n",
        "        for tag in soup.find_all([\"header\", \"footer\"]):\n",
        "            tag.decompose()\n",
        "        # Common nav/site chrome classes (adjust if needed for your HTML)\n",
        "        for selector in [\"nav\", \".header__main-Wrapper\", \".header__nav\", \".footer__info-Wrapper\",\n",
        "                         \".list__sponsor-Wrapper\", \".list__links-Wrapper\", \".breadcrumb\", \".header__breadcrumb-static\"]:\n",
        "            for el in soup.select(selector):\n",
        "                el.decompose()\n",
        "\n",
        "    # Use readability to pull main article-like content (fallback to full body)\n",
        "    try:\n",
        "        doc = Document(str(soup))\n",
        "        content_html = doc.summary(html_partial=True)\n",
        "        soup = BeautifulSoup(content_html, \"lxml\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Remove non-content tags\n",
        "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"canvas\", \"iframe\", \"form\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    # Get text, normalize whitespace\n",
        "    text = soup.get_text(separator=\"\\n\")\n",
        "\n",
        "    # Clean up: collapse repeated blank lines, trim lines\n",
        "    lines = [ln.strip() for ln in text.splitlines()]\n",
        "    lines = [ln for ln in lines if ln]  # drop empty lines\n",
        "    cleaned = \"\\n\".join(lines)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def get_links_from_page(html: str, base_url: str):\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    links = []\n",
        "    for a in soup.select(\"a[href]\"):\n",
        "        href = a.get(\"href\", \"\").strip()\n",
        "        if not href:\n",
        "            continue\n",
        "        absolute = urljoin(base_url, href)\n",
        "        absolute, _frag = urldefrag(absolute)  # remove #fragment\n",
        "        links.append(absolute)\n",
        "    return links\n",
        "\n",
        "def same_domain(url: str, domain: str) -> bool:\n",
        "    return urlparse(url).netloc.lower() == domain.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lu4hTHOWsGJ",
        "outputId": "96649476-a237-42f9-f091-7aa3b0c85892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTML file: c:\\Users\\grego\\Downloads\\New folder (10)\\Exterior.html\n",
            "Output text: c:\\Users\\grego\\Downloads\\New folder (10)\\Exterior_extracted.txt\n",
            "HTML exists: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Cell 3) Configuration â€” read from local Exterior.html\n",
        "# ============================================================\n",
        "# Path to the local HTML file (same folder as notebook / cwd)\n",
        "HTML_FILE = os.path.join(os.getcwd(), \"Exterior.html\")\n",
        "OUTPUT_DIR = os.getcwd()\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "OUTPUT_TXT = os.path.join(OUTPUT_DIR, \"Exterior_extracted.txt\")\n",
        "\n",
        "print(\"HTML file:\", HTML_FILE)\n",
        "print(\"Output text:\", OUTPUT_TXT)\n",
        "print(\"HTML exists:\", os.path.isfile(HTML_FILE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9t8EuxxWzDN",
        "outputId": "0cf9ec8e-ba0f-4670-c3d8-e4a4c2ec3f68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 40,546 characters from c:\\Users\\grego\\Downloads\\New folder (10)\\Exterior.html\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Cell 4) Load HTML from Exterior.html\n",
        "# ============================================================\n",
        "with open(HTML_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "print(f\"Loaded {len(html_content):,} characters from {HTML_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA2Q1lhWW2V2",
        "outputId": "059b9e66-63bb-4e92-b752-da2e01eb3b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Saved to: c:\\Users\\grego\\Downloads\\New folder (10)\\Exterior_extracted.txt\n",
            "Extracted length: 3362 characters\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Cell 5) Extract text (headers/footers removed) -> save .txt\n",
        "# ============================================================\n",
        "text = extract_visible_text(html_content, remove_header_footer=True)\n",
        "\n",
        "with open(OUTPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"Source: Exterior.html\\n\\n\")\n",
        "    f.write(text)\n",
        "\n",
        "print(\"Done.\")\n",
        "print(\"Saved to:\", OUTPUT_TXT)\n",
        "print(\"Extracted length:\", len(text), \"characters\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
