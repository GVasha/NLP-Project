{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BaA09R6fWfy7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Install dependencies (for Exterior.html extraction only)\n",
        "# ============================================================\n",
        "!pip -q install beautifulsoup4 lxml readability-lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CHKZbXcgWpbC"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Imports + extraction helper (Exterior.html only)\n",
        "# ============================================================\n",
        "import os\n",
        "from urllib.parse import urljoin, urldefrag\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def extract_visible_text(html: str, remove_header_footer: bool = True, base_url: str = None) -> str:\n",
        "    \"\"\"\n",
        "    Extract readable text from HTML (Exterior.html structure).\n",
        "    Removes modals, header, footer, nav. Targets #main content.\n",
        "    If base_url is set, preserves links as \"link text (full_url)\".\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # Remove modals and dialogs first (so they are not mistaken for main content)\n",
        "    for el in soup.select(\".modal, [role='dialog'], .modal-dialog\"):\n",
        "        el.decompose()\n",
        "\n",
        "    if remove_header_footer:\n",
        "        for tag in soup.find_all([\"header\", \"footer\"]):\n",
        "            tag.decompose()\n",
        "        for sel in [\"nav\", \"#searchbar\", \".breadcrumb\", \".migas\"]:\n",
        "            for el in soup.select(sel):\n",
        "                el.decompose()\n",
        "\n",
        "    # Target main content: Exterior.html (policia) uses id=\"main\"\n",
        "    main = soup.find(id=\"main\")\n",
        "    if main:\n",
        "        for skip in main.select(\"#calendarioFechaHora, .migas\"):\n",
        "            skip.decompose()\n",
        "        content_root = main\n",
        "    else:\n",
        "        content_root = soup\n",
        "\n",
        "    for tag in content_root([\"script\", \"style\", \"noscript\", \"svg\", \"canvas\", \"iframe\", \"form\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    if base_url:\n",
        "        for a in content_root.find_all(\"a\", href=True):\n",
        "            href = a.get(\"href\", \"\").strip()\n",
        "            if not href or href.startswith(\"javascript:\"):\n",
        "                continue\n",
        "            absolute, _ = urldefrag(urljoin(base_url, href))\n",
        "            link_text = a.get_text(separator=\" \", strip=True) or \"(link)\"\n",
        "            a.clear()\n",
        "            a.append(link_text + \" (\" + absolute + \")\")\n",
        "\n",
        "    text = content_root.get_text(separator=\"\\n\")\n",
        "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lu4hTHOWsGJ",
        "outputId": "96649476-a237-42f9-f091-7aa3b0c85892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTML file: c:\\Users\\grego\\Downloads\\New folder (10)\\Exterior.html\n",
            "Source URL: https://www.exteriores.gob.es/Embajadas/telaviv/en/ServiciosConsulares/Paginas/Consular/NIE.aspx\n",
            "Output: c:\\Users\\grego\\Downloads\\New folder (10)\\Exterior_extracted.txt\n",
            "Exists: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Configuration â€” Exterior.html only\n",
        "# ============================================================\n",
        "HTML_FILE = os.path.join(os.getcwd(), \"Exterior.html\")\n",
        "OUTPUT_DIR = os.getcwd()\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "OUTPUT_TXT = os.path.join(OUTPUT_DIR, \"Exterior_extracted.txt\")\n",
        "\n",
        "# Base URL for this Exterior.html (Spanish Police NIE page); change if your file is from a different site\n",
        "SOURCE_URL = \"https://sede.policia.gob.es/portalCiudadano/_ca-valencia/tramites_extranjeria_tramite_asignacion_nie.php\"\n",
        "\n",
        "print(\"HTML file:\", HTML_FILE)\n",
        "print(\"Source URL:\", SOURCE_URL)\n",
        "print(\"Output:\", OUTPUT_TXT)\n",
        "print(\"Exists:\", os.path.isfile(HTML_FILE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9t8EuxxWzDN",
        "outputId": "0cf9ec8e-ba0f-4670-c3d8-e4a4c2ec3f68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 64,217 characters from Exterior.html\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Load Exterior.html\n",
        "# ============================================================\n",
        "with open(HTML_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "print(f\"Loaded {len(html_content):,} characters from Exterior.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA2Q1lhWW2V2",
        "outputId": "059b9e66-63bb-4e92-b752-da2e01eb3b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Saved to: c:\\Users\\grego\\Downloads\\New folder (10)\\Exterior_extracted.txt\n",
            "Extracted length: 111 characters\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Extract text (header/footer removed, links preserved) -> save .txt\n",
        "# ============================================================\n",
        "text = extract_visible_text(html_content, remove_header_footer=True, base_url=SOURCE_URL)\n",
        "\n",
        "with open(OUTPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"Source: \" + SOURCE_URL + \"\\n\\n\")\n",
        "    f.write(text)\n",
        "\n",
        "print(\"Done. Saved to:\", OUTPUT_TXT)\n",
        "print(\"Extracted length:\", len(text), \"characters\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
